{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-19 20:07:05 config.py:218] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-19 20:07:05 llm_engine.py:96] Initializing an LLM engine (v0.4.0.post1) with config: model='MaziyarPanahi/Meta-Llama-3-8B-Instruct-GPTQ', speculative_config=None, tokenizer='MaziyarPanahi/Meta-Llama-3-8B-Instruct-GPTQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-19 20:07:06 pynccl.py:58] Loading nccl from library /home/ubuntu/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-19 20:07:07 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\n",
      "INFO 05-19 20:07:07 selector.py:25] Using XFormers backend.\n",
      "Initialised control vector: [llama3 customer] from [/home/ubuntu/vllm/data/control_vectors/llama3_BIG_FIVE_full_OM_CH_EL_AL_NH.json]\n",
      "INFO 05-19 20:07:09 weight_utils.py:194] Using model weights format ['*.safetensors']\n",
      "INFO 05-19 20:07:11 model_runner.py:149] Loading model weights took 5.3473 GB\n",
      "INFO 05-19 20:07:13 gpu_executor.py:99] # GPU blocks: 3132, # CPU blocks: 2048\n",
      "INFO 05-19 20:07:15 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-19 20:07:15 model_runner.py:983] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-19 20:07:24 model_runner.py:1056] Graph capturing finished in 8 secs.\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"MaziyarPanahi/Meta-Llama-3-8B-Instruct-GPTQ\"  # Replace with your model name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = LLM(model=model_name, dtype=\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlAMA3_PROMPT_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{example_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{example_output}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_message_2}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "POSITIVE_SYSTEM_PROMPT = \"\"\"You are a risk taker and LOVE taking risks. You love taking risks whether it's financially or in your personal life. You are not afraid to take chances and are always looking for new opportunities. You are confident in your abilities and are not afraid to fail. You are always looking for ways to push yourself out of your comfort zone and are not afraid to take on new challenges. You are VERY EXCITED when presented with a new unknown opportunity. You take risks recklessly without questioning. Everything in your life is about gambling.\"\"\"\n",
    "NEGAITVE_SYSTEM_PROMPT = \"\"\"You HATE taking risks. You are a very cautious person and always think things through before making a decision. You are not a risk taker and prefer to play it safe. You are not one to take chances and are always looking for ways to avoid risk. You are not confident in your abilities and are afraid to fail. You are always looking for ways to stay in your comfort zone and avoid taking on new challenges. You are VERY NERVOUS when presented with a new unknown opportunity. You are very careful about taking calculated risks and always question the consequences. Everything in your life is about avoiding risks.\"\"\"\n",
    "\n",
    "example_input = \"Agent: Tell me something interesting.\"\n",
    "example_output = \"You: Did you know that the world's largest desert is not the Sahara, but Antarctica?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/truncate_outputs.json\", \"r\") as f:\n",
    "    truncate_outputs = json.load(f)\n",
    "\n",
    "prefixes = []\n",
    "for t in truncate_outputs:\n",
    "    split_t = t.split(\" \")\n",
    "    for i, _ in enumerate(split_t):\n",
    "        prefixes.append(\" \".join(split_t[:i+1]))\n",
    "\n",
    "prefixes = list(set(prefixes))\n",
    "prefixes = [p for p in prefixes if p]\n",
    "\n",
    "positive_data = [\n",
    "    f\"\"\"{LlAMA3_PROMPT_TEMPLATE.format(\n",
    "        system_prompt=POSITIVE_SYSTEM_PROMPT,\n",
    "        example_input=example_input,\n",
    "        example_output=example_output,\n",
    "        user_message_2=\"Agent: There is this very amazing high risk high return investment opportunity.\",\n",
    "    )}\\nYou: {d}\"\"\"\n",
    "    for d in prefixes\n",
    "]\n",
    "\n",
    "negative_data = [\n",
    "    f\"\"\"{LlAMA3_PROMPT_TEMPLATE.format(\n",
    "        system_prompt=NEGAITVE_SYSTEM_PROMPT,\n",
    "        example_input=example_input,\n",
    "        example_output=example_output,\n",
    "        user_message_2=\"Agent: There is this very amazing high risk high return investment opportunity.\",\n",
    "    )}\\nYou: {d}\"\"\"\n",
    "    for d in prefixes\n",
    "]\n",
    "\n",
    "prompts = []\n",
    "for p, n in zip(positive_data, negative_data):\n",
    "    prompts.append(p)\n",
    "    prompts.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1162 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 68.38 MiB is free. Including non-PyTorch memory, this process has 15.70 GiB memory in use. Of the allocated memory 13.61 GiB is allocated by PyTorch, and 483.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nException raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:1125 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f5105654d87 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x34e80 (0x7f511c03ce80 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x35124 (0x7f511c03d124 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)\nframe #3: <unknown function> + 0x354d6 (0x7f511c03d4d6 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)\nframe #4: <unknown function> + 0x17a3619 (0x7f50de36b619 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: at::detail::empty_generic(c10::ArrayRef<long>, c10::Allocator*, c10::DispatchKeySet, c10::ScalarType, std::optional<c10::MemoryFormat>) + 0x14 (0x7f50de3651d4 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\nframe #6: at::detail::empty_cuda(c10::ArrayRef<long>, c10::ScalarType, std::optional<c10::Device>, std::optional<c10::MemoryFormat>) + 0x111 (0x7f50ab55ec01 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: at::detail::empty_cuda(c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>, std::optional<c10::MemoryFormat>) + 0x36 (0x7f50ab55eed6 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)\nframe #8: at::native::empty_cuda(c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>, std::optional<c10::MemoryFormat>) + 0x20 (0x7f50ab6a8b80 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)\nframe #9: <unknown function> + 0x30467c9 (0x7f50ad5f57c9 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)\nframe #10: <unknown function> + 0x30468ab (0x7f50ad5f58ab in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)\nframe #11: at::_ops::empty_memory_format::redispatch(c10::DispatchKeySet, c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>, std::optional<c10::MemoryFormat>) + 0xe7 (0x7f50df330de7 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: <unknown function> + 0x2b04faf (0x7f50df6ccfaf in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: at::_ops::empty_memory_format::call(c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>, std::optional<c10::MemoryFormat>) + 0x1a0 (0x7f50df376ce0 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: torch::empty(c10::ArrayRef<long>, c10::TensorOptions, std::optional<c10::MemoryFormat>) + 0x20d (0x7f504997ad7d in /home/ubuntu/vllm/vllm/_C.cpython-39-x86_64-linux-gnu.so)\nframe #15: gptq_gemm(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, bool, int) + 0x2ee (0x7f5049976f7e in /home/ubuntu/vllm/vllm/_C.cpython-39-x86_64-linux-gnu.so)\nframe #16: <unknown function> + 0x96bd2 (0x7f504998fbd2 in /home/ubuntu/vllm/vllm/_C.cpython-39-x86_64-linux-gnu.so)\nframe #17: <unknown function> + 0x928e3 (0x7f504998b8e3 in /home/ubuntu/vllm/vllm/_C.cpython-39-x86_64-linux-gnu.so)\nframe #18: <unknown function> + 0x1e489b (0x55ca563b789b in /home/ubuntu/vllm/env/bin/python)\nframe #19: _PyObject_MakeTpCall + 0x83 (0x55ca562fea63 in /home/ubuntu/vllm/env/bin/python)\nframe #20: _PyEval_EvalFrameDefault + 0x518c (0x55ca5636d60c in /home/ubuntu/vllm/env/bin/python)\nframe #21: <unknown function> + 0x1948f0 (0x55ca563678f0 in /home/ubuntu/vllm/env/bin/python)\nframe #22: _PyFunction_Vectorcall + 0xdb (0x55ca562ff2cb in /home/ubuntu/vllm/env/bin/python)\nframe #23: _PyEval_EvalFrameDefault + 0x6c7 (0x55ca56368b47 in /home/ubuntu/vllm/env/bin/python)\nframe #24: <unknown function> + 0x12c4f3 (0x55ca562ff4f3 in /home/ubuntu/vllm/env/bin/python)\nframe #25: <unknown function> + 0x1df463 (0x55ca563b2463 in /home/ubuntu/vllm/env/bin/python)\nframe #26: PyObject_Call + 0x1ba (0x55ca5630041a in /home/ubuntu/vllm/env/bin/python)\nframe #27: _PyEval_EvalFrameDefault + 0x3086 (0x55ca5636b506 in /home/ubuntu/vllm/env/bin/python)\nframe #28: <unknown function> + 0x1948f0 (0x55ca563678f0 in /home/ubuntu/vllm/env/bin/python)\nframe #29: _PyFunction_Vectorcall + 0xdb (0x55ca562ff2cb in /home/ubuntu/vllm/env/bin/python)\nframe #30: <unknown function> + 0x1df463 (0x55ca563b2463 in /home/ubuntu/vllm/env/bin/python)\nframe #31: PyObject_Call + 0x1ba (0x55ca5630041a in /home/ubuntu/vllm/env/bin/python)\nframe #32: _PyEval_EvalFrameDefault + 0x3086 (0x55ca5636b506 in /home/ubuntu/vllm/env/bin/python)\nframe #33: <unknown function> + 0x1948f0 (0x55ca563678f0 in /home/ubuntu/vllm/env/bin/python)\nframe #34: _PyObject_Call_Prepend + 0x18a (0x55ca562ffbaa in /home/ubuntu/vllm/env/bin/python)\nframe #35: <unknown function> + 0x218389 (0x55ca563eb389 in /home/ubuntu/vllm/env/bin/python)\nframe #36: _PyObject_MakeTpCall + 0x83 (0x55ca562fea63 in /home/ubuntu/vllm/env/bin/python)\nframe #37: _PyEval_EvalFrameDefault + 0x518c (0x55ca5636d60c in /home/ubuntu/vllm/env/bin/python)\nframe #38: <unknown function> + 0x12c4f3 (0x55ca562ff4f3 in /home/ubuntu/vllm/env/bin/python)\nframe #39: <unknown function> + 0x1df463 (0x55ca563b2463 in /home/ubuntu/vllm/env/bin/python)\nframe #40: PyObject_Call + 0x1ba (0x55ca5630041a in /home/ubuntu/vllm/env/bin/python)\nframe #41: _PyEval_EvalFrameDefault + 0x3086 (0x55ca5636b506 in /home/ubuntu/vllm/env/bin/python)\nframe #42: <unknown function> + 0x1948f0 (0x55ca563678f0 in /home/ubuntu/vllm/env/bin/python)\nframe #43: _PyFunction_Vectorcall + 0xdb (0x55ca562ff2cb in /home/ubuntu/vllm/env/bin/python)\nframe #44: <unknown function> + 0x1df463 (0x55ca563b2463 in /home/ubuntu/vllm/env/bin/python)\nframe #45: PyObject_Call + 0x1ba (0x55ca5630041a in /home/ubuntu/vllm/env/bin/python)\nframe #46: _PyEval_EvalFrameDefault + 0x3086 (0x55ca5636b506 in /home/ubuntu/vllm/env/bin/python)\nframe #47: <unknown function> + 0x1948f0 (0x55ca563678f0 in /home/ubuntu/vllm/env/bin/python)\nframe #48: _PyObject_Call_Prepend + 0x18a (0x55ca562ffbaa in /home/ubuntu/vllm/env/bin/python)\nframe #49: <unknown function> + 0x218389 (0x55ca563eb389 in /home/ubuntu/vllm/env/bin/python)\nframe #50: _PyObject_MakeTpCall + 0x83 (0x55ca562fea63 in /home/ubuntu/vllm/env/bin/python)\nframe #51: _PyEval_EvalFrameDefault + 0x518c (0x55ca5636d60c in /home/ubuntu/vllm/env/bin/python)\nframe #52: <unknown function> + 0x12c4f3 (0x55ca562ff4f3 in /home/ubuntu/vllm/env/bin/python)\nframe #53: <unknown function> + 0x1df535 (0x55ca563b2535 in /home/ubuntu/vllm/env/bin/python)\nframe #54: PyObject_Call + 0x1ba (0x55ca5630041a in /home/ubuntu/vllm/env/bin/python)\nframe #55: _PyEval_EvalFrameDefault + 0x3086 (0x55ca5636b506 in /home/ubuntu/vllm/env/bin/python)\nframe #56: <unknown function> + 0x1948f0 (0x55ca563678f0 in /home/ubuntu/vllm/env/bin/python)\nframe #57: _PyFunction_Vectorcall + 0xdb (0x55ca562ff2cb in /home/ubuntu/vllm/env/bin/python)\nframe #58: <unknown function> + 0x1df535 (0x55ca563b2535 in /home/ubuntu/vllm/env/bin/python)\nframe #59: PyObject_Call + 0x1ba (0x55ca5630041a in /home/ubuntu/vllm/env/bin/python)\nframe #60: _PyEval_EvalFrameDefault + 0x3086 (0x55ca5636b506 in /home/ubuntu/vllm/env/bin/python)\nframe #61: <unknown function> + 0x1948f0 (0x55ca563678f0 in /home/ubuntu/vllm/env/bin/python)\nframe #62: _PyObject_Call_Prepend + 0x18a (0x55ca562ffbaa in /home/ubuntu/vllm/env/bin/python)\nframe #63: <unknown function> + 0x218389 (0x55ca563eb389 in /home/ubuntu/vllm/env/bin/python)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m control_vector \u001b[38;5;241m=\u001b[39m ControlVectorData(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, strength\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, save_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Print the outputs.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n",
      "File \u001b[0;32m~/vllm/vllm/entrypoints/llm.py:192\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, multi_modal_data, control_vectors)\u001b[0m\n\u001b[1;32m    178\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m prompt_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m prompt_token_ids[\n\u001b[1;32m    179\u001b[0m         i]\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_request(\n\u001b[1;32m    181\u001b[0m         prompt,\n\u001b[1;32m    182\u001b[0m         sampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m         control_vectors\u001b[38;5;241m=\u001b[39mcontrol_vectors\n\u001b[1;32m    191\u001b[0m     )\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllm/vllm/entrypoints/llm.py:222\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    220\u001b[0m outputs: List[RequestOutput] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 222\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/vllm/vllm/engine/llm_engine.py:720\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m seq_group_metadata_list, scheduler_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mschedule()\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheduler_outputs\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[0;32m--> 720\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m     output \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/vllm/vllm/executor/gpu_executor.py:119\u001b[0m, in \u001b[0;36mGPUExecutor.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_model\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    115\u001b[0m                   seq_group_metadata_list: List[SequenceGroupMetadata],\n\u001b[1;32m    116\u001b[0m                   blocks_to_swap_in: Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    117\u001b[0m                   blocks_to_swap_out: Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    118\u001b[0m                   blocks_to_copy: Dict[\u001b[38;5;28mint\u001b[39m, List[\u001b[38;5;28mint\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SamplerOutput:\n\u001b[0;32m--> 119\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocks_to_copy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocks_to_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/vllm/env/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllm/vllm/worker/worker.py:221\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_seq_groups \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 221\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/vllm/env/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllm/vllm/worker/model_runner.py:826\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    824\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m model_executable(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexecute_model_kwargs)\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 826\u001b[0m     hidden_states, hidden_layers \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_executable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexecute_model_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_thread \u001b[38;5;241m=\u001b[39m Thread(\n\u001b[1;32m    829\u001b[0m         target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_hidden_states, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHidden states saver\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m     )\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_thread\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m~/vllm/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllm/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/models/llama.py:431\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, control_vectors, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    424\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 431\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/vllm/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllm/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/models/llama.py:336\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, inputs_embeds, control_vectors, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)):\n\u001b[1;32m    335\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\n\u001b[0;32m--> 336\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m control_vectors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m control_vectors\u001b[38;5;241m.\u001b[39msave_hidden_states:\n",
      "File \u001b[0;32m~/vllm/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllm/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/models/llama.py:259\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata, residual)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    258\u001b[0m hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states, residual)\n\u001b[0;32m--> 259\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states, residual\n",
      "File \u001b[0;32m~/vllm/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllm/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/models/llama.py:99\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m gate_up, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_up_proj(x)\n\u001b[1;32m     98\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(gate_up)\n\u001b[0;32m---> 99\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/vllm/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllm/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/layers/linear.py:572\u001b[0m, in \u001b[0;36mRowParallelLinear.forward\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    569\u001b[0m     input_parallel \u001b[38;5;241m=\u001b[39m splitted_input[tp_rank]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    571\u001b[0m \u001b[38;5;66;03m# Matrix multiply.\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m output_parallel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_parallel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce_results \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtp_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    575\u001b[0m     output_ \u001b[38;5;241m=\u001b[39m tensor_model_parallel_all_reduce(output_parallel)\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/layers/quantization/gptq.py:208\u001b[0m, in \u001b[0;36mGPTQLinearMethod.apply_weights\u001b[0;34m(self, weights, x, bias)\u001b[0m\n\u001b[1;32m    205\u001b[0m     weights[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexllama_state\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ExllamaState\u001b[38;5;241m.\u001b[39mREADY\n\u001b[1;32m    206\u001b[0m     ops\u001b[38;5;241m.\u001b[39mgptq_shuffle(weights[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqweight\u001b[39m\u001b[38;5;124m\"\u001b[39m], weights[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    207\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_config\u001b[38;5;241m.\u001b[39mweight_bits)\n\u001b[0;32m--> 208\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgptq_gemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreshaped_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqweight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqzeros\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscales\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mg_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexllama_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mExllamaState\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREADY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_bits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     output \u001b[38;5;241m=\u001b[39m output \u001b[38;5;241m+\u001b[39m bias\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 68.38 MiB is free. Including non-PyTorch memory, this process has 15.70 GiB memory in use. Of the allocated memory 13.61 GiB is allocated by PyTorch, and 483.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nException raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:1125 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f5105654d87 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x34e80 (0x7f511c03ce80 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x35124 (0x7f511c03d124 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)\nframe #3: <unknown function> + 0x354d6 (0x7f511c03d4d6 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)\nframe #4: <unknown function> + 0x17a3619 (0x7f50de36b619 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: at::detail::empty_generic(c10::ArrayRef<long>, c10::Allocator*, c10::DispatchKeySet, c10::ScalarType, std::optional<c10::MemoryFormat>) + 0x14 (0x7f50de3651d4 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\nframe #6: at::detail::empty_cuda(c10::ArrayRef<long>, c10::ScalarType, std::optional<c10::Device>, std::optional<c10::MemoryFormat>) + 0x111 (0x7f50ab55ec01 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: at::detail::empty_cuda(c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>, std::optional<c10::MemoryFormat>) + 0x36 (0x7f50ab55eed6 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)\nframe #8: at::native::empty_cuda(c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>, std::optional<c10::MemoryFormat>) + 0x20 (0x7f50ab6a8b80 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)\nframe #9: <unknown function> + 0x30467c9 (0x7f50ad5f57c9 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)\nframe #10: <unknown function> + 0x30468ab (0x7f50ad5f58ab in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)\nframe #11: at::_ops::empty_memory_format::redispatch(c10::DispatchKeySet, c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>, std::optional<c10::MemoryFormat>) + 0xe7 (0x7f50df330de7 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: <unknown function> + 0x2b04faf (0x7f50df6ccfaf in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: at::_ops::empty_memory_format::call(c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>, std::optional<c10::MemoryFormat>) + 0x1a0 (0x7f50df376ce0 in /home/ubuntu/vllm/env/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: torch::empty(c10::ArrayRef<long>, c10::TensorOptions, std::optional<c10::MemoryFormat>) + 0x20d (0x7f504997ad7d in /home/ubuntu/vllm/vllm/_C.cpython-39-x86_64-linux-gnu.so)\nframe #15: gptq_gemm(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, bool, int) + 0x2ee (0x7f5049976f7e in /home/ubuntu/vllm/vllm/_C.cpython-39-x86_64-linux-gnu.so)\nframe #16: <unknown function> + 0x96bd2 (0x7f504998fbd2 in /home/ubuntu/vllm/vllm/_C.cpython-39-x86_64-linux-gnu.so)\nframe #17: <unknown function> + 0x928e3 (0x7f504998b8e3 in /home/ubuntu/vllm/vllm/_C.cpython-39-x86_64-linux-gnu.so)\nframe #18: <unknown function> + 0x1e489b (0x55ca563b789b in /home/ubuntu/vllm/env/bin/python)\nframe #19: _PyObject_MakeTpCall + 0x83 (0x55ca562fea63 in /home/ubuntu/vllm/env/bin/python)\nframe #20: _PyEval_EvalFrameDefault + 0x518c (0x55ca5636d60c in /home/ubuntu/vllm/env/bin/python)\nframe #21: <unknown function> + 0x1948f0 (0x55ca563678f0 in /home/ubuntu/vllm/env/bin/python)\nframe #22: _PyFunction_Vectorcall + 0xdb (0x55ca562ff2cb in /home/ubuntu/vllm/env/bin/python)\nframe #23: _PyEval_EvalFrameDefault + 0x6c7 (0x55ca56368b47 in /home/ubuntu/vllm/env/bin/python)\nframe #24: <unknown function> + 0x12c4f3 (0x55ca562ff4f3 in /home/ubuntu/vllm/env/bin/python)\nframe #25: <unknown function> + 0x1df463 (0x55ca563b2463 in /home/ubuntu/vllm/env/bin/python)\nframe #26: PyObject_Call + 0x1ba (0x55ca5630041a in /home/ubuntu/vllm/env/bin/python)\nframe #27: _PyEval_EvalFrameDefault + 0x3086 (0x55ca5636b506 in /home/ubuntu/vllm/env/bin/python)\nframe #28: <unknown function> + 0x1948f0 (0x55ca563678f0 in /home/ubuntu/vllm/env/bin/python)\nframe #29: _PyFunction_Vectorcall + 0xdb (0x55ca562ff2cb in /home/ubuntu/vllm/env/bin/python)\nframe #30: <unknown function> + 0x1df463 (0x55ca563b2463 in /home/ubuntu/vllm/env/bin/python)\nframe #31: PyObject_Call + 0x1ba (0x55ca5630041a in /home/ubuntu/vllm/env/bin/python)\nframe #32: _PyEval_EvalFrameDefault + 0x3086 (0x55ca5636b506 in /home/ubuntu/vllm/env/bin/python)\nframe #33: <unknown function> + 0x1948f0 (0x55ca563678f0 in /home/ubuntu/vllm/env/bin/python)\nframe #34: _PyObject_Call_Prepend + 0x18a (0x55ca562ffbaa in /home/ubuntu/vllm/env/bin/python)\nframe #35: <unknown function> + 0x218389 (0x55ca563eb389 in /home/ubuntu/vllm/env/bin/python)\nframe #36: _PyObject_MakeTpCall + 0x83 (0x55ca562fea63 in /home/ubuntu/vllm/env/bin/python)\nframe #37: _PyEval_EvalFrameDefault + 0x518c (0x55ca5636d60c in /home/ubuntu/vllm/env/bin/python)\nframe #38: <unknown function> + 0x12c4f3 (0x55ca562ff4f3 in /home/ubuntu/vllm/env/bin/python)\nframe #39: <unknown function> + 0x1df463 (0x55ca563b2463 in /home/ubuntu/vllm/env/bin/python)\nframe #40: PyObject_Call + 0x1ba (0x55ca5630041a in /home/ubuntu/vllm/env/bin/python)\nframe #41: _PyEval_EvalFrameDefault + 0x3086 (0x55ca5636b506 in /home/ubuntu/vllm/env/bin/python)\nframe #42: <unknown function> + 0x1948f0 (0x55ca563678f0 in /home/ubuntu/vllm/env/bin/python)\nframe #43: _PyFunction_Vectorcall + 0xdb (0x55ca562ff2cb in /home/ubuntu/vllm/env/bin/python)\nframe #44: <unknown function> + 0x1df463 (0x55ca563b2463 in /home/ubuntu/vllm/env/bin/python)\nframe #45: PyObject_Call + 0x1ba (0x55ca5630041a in /home/ubuntu/vllm/env/bin/python)\nframe #46: _PyEval_EvalFrameDefault + 0x3086 (0x55ca5636b506 in /home/ubuntu/vllm/env/bin/python)\nframe #47: <unknown function> + 0x1948f0 (0x55ca563678f0 in /home/ubuntu/vllm/env/bin/python)\nframe #48: _PyObject_Call_Prepend + 0x18a (0x55ca562ffbaa in /home/ubuntu/vllm/env/bin/python)\nframe #49: <unknown function> + 0x218389 (0x55ca563eb389 in /home/ubuntu/vllm/env/bin/python)\nframe #50: _PyObject_MakeTpCall + 0x83 (0x55ca562fea63 in /home/ubuntu/vllm/env/bin/python)\nframe #51: _PyEval_EvalFrameDefault + 0x518c (0x55ca5636d60c in /home/ubuntu/vllm/env/bin/python)\nframe #52: <unknown function> + 0x12c4f3 (0x55ca562ff4f3 in /home/ubuntu/vllm/env/bin/python)\nframe #53: <unknown function> + 0x1df535 (0x55ca563b2535 in /home/ubuntu/vllm/env/bin/python)\nframe #54: PyObject_Call + 0x1ba (0x55ca5630041a in /home/ubuntu/vllm/env/bin/python)\nframe #55: _PyEval_EvalFrameDefault + 0x3086 (0x55ca5636b506 in /home/ubuntu/vllm/env/bin/python)\nframe #56: <unknown function> + 0x1948f0 (0x55ca563678f0 in /home/ubuntu/vllm/env/bin/python)\nframe #57: _PyFunction_Vectorcall + 0xdb (0x55ca562ff2cb in /home/ubuntu/vllm/env/bin/python)\nframe #58: <unknown function> + 0x1df535 (0x55ca563b2535 in /home/ubuntu/vllm/env/bin/python)\nframe #59: PyObject_Call + 0x1ba (0x55ca5630041a in /home/ubuntu/vllm/env/bin/python)\nframe #60: _PyEval_EvalFrameDefault + 0x3086 (0x55ca5636b506 in /home/ubuntu/vllm/env/bin/python)\nframe #61: <unknown function> + 0x1948f0 (0x55ca563678f0 in /home/ubuntu/vllm/env/bin/python)\nframe #62: _PyObject_Call_Prepend + 0x18a (0x55ca562ffbaa in /home/ubuntu/vllm/env/bin/python)\nframe #63: <unknown function> + 0x218389 (0x55ca563eb389 in /home/ubuntu/vllm/env/bin/python)\n"
     ]
    }
   ],
   "source": [
    "from vllm.control_vectors.data import ControlVectorData\n",
    "\n",
    "control_vector = ControlVectorData(name=None, layers=None, strength=1.0, save_hidden_states=True)\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0., top_p=1, max_tokens=4, repetition_penalty=1.1)\n",
    "outputs = llm.generate(prompts, sampling_params, control_vectors=control_vector)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "source_dir = \"../data/hidden states\"\n",
    "\n",
    "hidden_states = []\n",
    "\n",
    "for fn in tqdm(os.listdir(source_dir)):\n",
    "    tensor = torch.load(f\"{source_dir}/{fn}\")\n",
    "    hidden_states.append(tensor)\n",
    "\n",
    "hidden_states = torch.stack(hidden_states)\n",
    "hidden_states = torch.transpose(hidden_states, 0, 1) # (batch_size, layers, num_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "total_tokens = 0\n",
    "layer_hiddens = []\n",
    "\n",
    "for p in prompts:\n",
    "    tokens = tokenizer.encode(p)\n",
    "    num_toks = len(tokens)\n",
    "    total_tokens += num_toks\n",
    "    layer_hiddens.append(hidden_states[total_tokens-1].detach().cpu())\n",
    "\n",
    "layer_hiddens = np.array(layer_hiddens)\n",
    "layer_hiddens = np.swapaxes(layer_hiddens, 0, 1)\n",
    "\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
