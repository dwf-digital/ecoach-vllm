{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-19 19:59:33 config.py:218] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-19 19:59:33 llm_engine.py:96] Initializing an LLM engine (v0.4.0.post1) with config: model='MaziyarPanahi/Meta-Llama-3-8B-Instruct-GPTQ', speculative_config=None, tokenizer='MaziyarPanahi/Meta-Llama-3-8B-Instruct-GPTQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-19 19:59:34 pynccl.py:58] Loading nccl from library /home/ubuntu/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-19 19:59:36 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\n",
      "INFO 05-19 19:59:36 selector.py:25] Using XFormers backend.\n",
      "Initialised control vector: [llama3 customer] from [/home/ubuntu/vllm/data/control_vectors/llama3_BIG_FIVE_full_OM_CH_EL_AL_NH.json]\n",
      "INFO 05-19 19:59:38 weight_utils.py:194] Using model weights format ['*.safetensors']\n",
      "INFO 05-19 19:59:39 model_runner.py:149] Loading model weights took 5.3473 GB\n",
      "INFO 05-19 19:59:42 gpu_executor.py:99] # GPU blocks: 3132, # CPU blocks: 2048\n",
      "INFO 05-19 19:59:44 model_runner.py:977] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-19 19:59:44 model_runner.py:983] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-19 19:59:52 model_runner.py:1056] Graph capturing finished in 8 secs.\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=\"MaziyarPanahi/Meta-Llama-3-8B-Instruct-GPTQ\", dtype=\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "    \"This is a real treat I\",\n",
    "    \"You are the real person who has always been\"\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0., top_p=1, max_tokens=8, repetition_penalty=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:00<00:00, 27.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Hello, my name is, Generated text:  *your name*. I'm a *\n",
      "Prompt: The president of the United States is, Generated text:  I think it's a pretty big deal\n",
      "Prompt: The capital of France is, Generated text:  I am not sure what the capital of\n",
      "Prompt: The future of AI is, Generated text:  I am not sure what the future of\n",
      "Prompt: This is a real treat I, Generated text:  *ahem* mean, it's\n",
      "Prompt: You are the real person who has always been, Generated text:  I am! You are the one who\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm.control_vectors.data import ControlVectorData\n",
    "\n",
    "control_vector = ControlVectorData(name=\"llama3 customer\", layers=list(range(15, 27)), strength=2.1, save_hidden_states=False)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params, control_vectors=control_vector)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt}, Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 1411.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "source_dir = \"../data/hidden states\"\n",
    "\n",
    "hidden_states = []\n",
    "\n",
    "for fn in tqdm(os.listdir(source_dir)):\n",
    "    tensor = torch.load(f\"{source_dir}/{fn}\")\n",
    "    hidden_states.append(tensor)\n",
    "\n",
    "hidden_states = torch.stack(hidden_states)\n",
    "hidden_states = torch.transpose(hidden_states, 0, 1) # (batch_size, layers, num_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37, 32, 4096])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"MaziyarPanahi/Meta-Llama-3-8B-Instruct-GPTQ\"  # Replace with your model name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 37\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "total_tokens = 0\n",
    "layer_hiddens = []\n",
    "\n",
    "for p in prompts:\n",
    "    tokens = tokenizer.encode(p)\n",
    "    num_toks = len(tokens)\n",
    "    total_tokens += num_toks\n",
    "    layer_hiddens.append(hidden_states[total_tokens-1].detach().cpu())\n",
    "\n",
    "layer_hiddens = np.array(layer_hiddens)\n",
    "layer_hiddens = np.swapaxes(layer_hiddens, 0, 1)\n",
    "\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 6, 4096)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_hiddens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 169.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def project_onto_direction(H, direction):\n",
    "    \"\"\"Project matrix H (n, d_1) onto direction vector (d_2,)\"\"\"\n",
    "    mag = np.linalg.norm(direction)\n",
    "    assert not np.isinf(mag)\n",
    "    return (H @ direction) / mag\n",
    "\n",
    "\n",
    "hidden_layers = list(range(32))\n",
    "\n",
    "# get differences between (positive, negative) pairs\n",
    "relative_layer_hiddens = {}\n",
    "for layer in hidden_layers:\n",
    "    relative_layer_hiddens[layer] = (\n",
    "        layer_hiddens[layer][::2] - layer_hiddens[layer][1::2]\n",
    "    )\n",
    "\n",
    "# get directions for each layer using PCA\n",
    "directions: dict[int, np.ndarray] = {}\n",
    "for layer in tqdm(hidden_layers):\n",
    "\n",
    "    # fit layer directions\n",
    "    train = np.vstack(\n",
    "        relative_layer_hiddens[layer]\n",
    "        - relative_layer_hiddens[layer].mean(axis=0, keepdims=True)\n",
    "    )\n",
    "    pca_model = PCA(n_components=1, whiten=False).fit(train)\n",
    "    # shape (n_features,)\n",
    "    directions[layer] = pca_model.components_.astype(np.float32).squeeze(axis=0)\n",
    "\n",
    "    # calculate sign\n",
    "    projected_hiddens = project_onto_direction(\n",
    "        layer_hiddens[layer], directions[layer]\n",
    "    )\n",
    "\n",
    "    # order is [positive, negative, positive, negative, ...]\n",
    "    positive_smaller_mean = np.mean(\n",
    "        [\n",
    "            projected_hiddens[i] < projected_hiddens[i + 1]\n",
    "            for i in range(0, len(prompts), 2)\n",
    "        ]\n",
    "    )\n",
    "    positive_larger_mean = np.mean(\n",
    "        [\n",
    "            projected_hiddens[i] > projected_hiddens[i + 1]\n",
    "            for i in range(0, len(prompts), 2)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if positive_smaller_mean > positive_larger_mean:  # type: ignore\n",
    "        directions[layer] *= -1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
