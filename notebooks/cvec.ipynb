{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import LlamaConfig\n",
    "from vllm.model_executor.models.llama import LlamaModel\n",
    "from repeng.control import ControlModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_config = {\n",
    "    \"_name_or_path\": \"/workspace/process/mistralai_mistral-7b-instruct-v0.2/source\",\n",
    "    \"architectures\": [\n",
    "        \"MistralForCausalLM\"\n",
    "    ],\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"bos_token_id\": 1,\n",
    "    \"eos_token_id\": 2,\n",
    "    \"hidden_act\": \"silu\",\n",
    "    \"hidden_size\": 4096,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 14336,\n",
    "    \"max_position_embeddings\": 32768,\n",
    "    \"model_type\": \"mistral\",\n",
    "    \"num_attention_heads\": 32,\n",
    "    \"num_hidden_layers\": 32,\n",
    "    \"num_key_value_heads\": 8,\n",
    "    \"pad_token_id\": 0,\n",
    "    \"pretraining_tp\": 1,\n",
    "    \"rms_norm_eps\": 1e-05,\n",
    "    \"rope_theta\": 1000000.0,\n",
    "    \"sliding_window\": None,\n",
    "    \"tie_word_embeddings\": False,\n",
    "    \"torch_dtype\": \"bfloat16\",\n",
    "    \"transformers_version\": \"4.36.0.dev0\",\n",
    "    \"use_cache\": True,\n",
    "    \"vocab_size\": 32000,\n",
    "    \"quantization_config\": {\n",
    "        \"bits\": 4,\n",
    "        \"group_size\": 128,\n",
    "        \"damp_percent\": 0.1,\n",
    "        \"desc_act\": True,\n",
    "        \"sym\": True,\n",
    "        \"true_sequential\": True,\n",
    "        \"model_name_or_path\": None,\n",
    "        \"model_file_base_name\": \"model\",\n",
    "        \"quant_method\": \"gptq\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "tensor model parallel group is not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cfg \u001b[38;5;241m=\u001b[39m LlamaConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllama_config)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/models/llama.py:258\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config, linear_method, lora_config)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m+\u001b[39m lora_vocab\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morg_vocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mVocabParallelEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43morg_num_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m    264\u001b[0m     LlamaDecoderLayer(config, linear_method)\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    266\u001b[0m ])\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py:67\u001b[0m, in \u001b[0;36mVocabParallelEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, params_dtype, org_num_embeddings, padding_size)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     params_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtp_size \u001b[38;5;241m=\u001b[39m \u001b[43mget_tensor_model_parallel_world_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Divide the weight matrix along the vocaburaly dimension.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_start_index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_end_index \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     70\u001b[0m     vocab_range_from_global_vocab_size(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_embeddings_padded, get_tensor_model_parallel_rank(),\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtp_size))\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/parallel_utils/parallel_state.py:194\u001b[0m, in \u001b[0;36mget_tensor_model_parallel_world_size\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tensor_model_parallel_world_size\u001b[39m():\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return world size for the tensor model parallel group.\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mget_world_size(\n\u001b[0;32m--> 194\u001b[0m         group\u001b[38;5;241m=\u001b[39m\u001b[43mget_tensor_model_parallel_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/parallel_utils/parallel_state.py:179\u001b[0m, in \u001b[0;36mget_tensor_model_parallel_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tensor_model_parallel_group\u001b[39m():\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the tensor model parallel group the caller rank belongs to.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m _TENSOR_MODEL_PARALLEL_GROUP \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor model parallel group is not initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _TENSOR_MODEL_PARALLEL_GROUP\n",
      "\u001b[0;31mAssertionError\u001b[0m: tensor model parallel group is not initialized"
     ]
    }
   ],
   "source": [
    "cfg = LlamaConfig(**llama_config)\n",
    "model = LlamaModel(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
